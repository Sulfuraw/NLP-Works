{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction: \n",
    "This project is about completion (prediction) and categorization of words.\n",
    "\n",
    "It involves the use of:\n",
    "\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import BracketParseCorpusReader\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import bigrams\n",
    "import numpy as np\n",
    "\n",
    "corpus = BracketParseCorpusReader(root=\"corpora\", fileids=[\"p1_train.txt\"])\n",
    "words = corpus.words()\n",
    "sentences = corpus.sents()\n",
    "parsed_texts = corpus.parsed_sents()\n",
    "test = BracketParseCorpusReader(root=\"corpora\", fileids=[\"p1_test.txt\"])\n",
    "test_set = test.sents()\n",
    "vocab = Vocabulary(words, unk_cutoff=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little checks over the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our vocabulary, we have put all the words of the corpus that appear at least 3 times. Other present words are replace by the token \\<UNK> as of every other words not present in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 last words of the vocabulary with their number of appearence in the corpus:\n",
      "  [('wry', 6), ('year', 32), ('years', 8), ('yes', 3), ('yet', 11), ('you', 166), ('young', 12), ('your', 31), ('yourself', 5), ('zippy', 3)]\n"
     ]
    }
   ],
   "source": [
    "word_num = []\n",
    "for word in sorted(vocab)[-10:]:\n",
    "    word_num.append((word, vocab[word]))\n",
    "print(f\"The 10 last words of the vocabulary with their number of appearence in the corpus:\\n  {word_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unkown words: 7352\n",
      "Out of vocabulary (OOV) rate: 19.295%\n"
     ]
    }
   ],
   "source": [
    "count_unk = 0\n",
    "for word in words:\n",
    "    if word not in vocab:\n",
    "        count_unk += 1\n",
    "oov = (count_unk/len(words))*100\n",
    "\n",
    "print(f\"The number of unkown words: {count_unk}\")\n",
    "print(f\"Out of vocabulary (OOV) rate: {round(oov, 3)}%\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the bigrams of the 10th sentence:\n",
      "  [('<s>', 'It'), ('It', \"'s\"), (\"'s\", 'a'), ('a', 'coming-of-age'), ('coming-of-age', 'story'), ('story', 'we'), ('we', \"'ve\"), (\"'ve\", 'all'), ('all', 'seen'), ('seen', 'bits'), ('bits', 'of'), ('of', 'in'), ('in', 'other'), ('other', 'films'), ('films', '--'), ('--', 'but'), ('but', 'it'), ('it', \"'s\"), (\"'s\", 'rarely'), ('rarely', 'been'), ('been', 'told'), ('told', 'with'), ('with', 'such'), ('such', 'affecting'), ('affecting', 'grace'), ('grace', 'and'), ('and', 'cultural'), ('cultural', 'specificity'), ('specificity', '.'), ('.', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "sentence_10_bi = list(bigrams(list(pad_both_ends(sentences[9], n=2))))\n",
    "print(f\"All the bigrams of the 10th sentence:\\n  {sentence_10_bi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text completion part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum-likelihood estimation (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilty of having a word (w) if we know the history (h) (context).\n",
    "\n",
    "Since we don't know the probability, we estimate it by counting over the corpus.\n",
    "\n",
    "The counting function is noted $C(*)$\n",
    "\n",
    "$\\^P(w|h) = \\frac{C(h,w)}{C(h)}$\n",
    "\n",
    "(if the $C(h) <= 0$ then $\\^P(w|h) = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is needed to compute both mle and mle_laplace\n",
    "\n",
    "# bi_count == C(h,w)\n",
    "bi_count = {}\n",
    "# mle will be == P(w|h) after computation, same for mle_laplace with the smoothing\n",
    "# Not computed here, only initialised\n",
    "mle = {}\n",
    "mle_laplace = {}\n",
    "for sentence in range(len(sentences)):\n",
    "    for bi in list(bigrams(list(pad_both_ends(sentences[sentence], n=2)))):\n",
    "        if bi[0] == '<s>':\n",
    "            bi = (bi[0], vocab.lookup(bi[1]))\n",
    "        elif bi[1] == '</s>':\n",
    "            bi = (vocab.lookup(bi[0]), bi[1])\n",
    "        else:\n",
    "            bi = (vocab.lookup(bi[0]), vocab.lookup(bi[1]))\n",
    "        if bi_count.get(bi[0], -1) == -1:\n",
    "            bi_count[bi[0]] = {}\n",
    "            mle[bi[0]] = {}\n",
    "            mle_laplace[bi[0]] = {}\n",
    "        count_bi = bi_count[bi[0]].get(bi[1], 0)\n",
    "        bi_count[bi[0]][bi[1]] = count_bi+1\n",
    "\n",
    "# h_count == C(h)\n",
    "h_count = {}\n",
    "for first_word in bi_count.keys():    \n",
    "    count_next = 0\n",
    "    for i in bi_count[first_word]:\n",
    "        count_next += bi_count[first_word][i]\n",
    "    h_count[first_word] = count_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mle\n",
    "\n",
    "best_nexts_key_mle = {}\n",
    "for first_word in bi_count.keys():    \n",
    "    for second_word in bi_count[first_word].keys():\n",
    "        previous = bi_count[first_word][second_word]\n",
    "        mle[first_word][second_word] = previous/h_count[first_word]\n",
    "        \n",
    "    key_next = list(mle[first_word].keys())\n",
    "    key_next = sorted(key_next, key= mle[first_word].__getitem__)\n",
    "    best_nexts_key_mle[first_word] = key_next   \n",
    "\n",
    "def complete_next_word_mle(target):\n",
    "    ret = {}\n",
    "    nexts = best_nexts_key_mle[target]\n",
    "    for i in range(5):\n",
    "        ret[nexts[-i-1]] = round(mle[target][nexts[-i-1]], 4)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most probable completion after a target word (<s> is the start of a sentence):\n",
      "\n",
      "After the target <s>:\n",
      "- <UNK>: 0.268\n",
      "- The: 0.1105\n",
      "- A: 0.0915\n",
      "- It: 0.075\n",
      "- This: 0.0345\n",
      "After the target love:\n",
      "- story: 0.1923\n",
      "- <UNK>: 0.1538\n",
      "- ,: 0.1154\n",
      "- and: 0.0769\n",
      "- with: 0.0769\n"
     ]
    }
   ],
   "source": [
    "# Run example\n",
    "\n",
    "print(\"The 5 most probable completion after a target word (<s> is the start of a sentence):\\n\")\n",
    "for target in [\"<s>\", \"love\"]:\n",
    "    print(f\"After the target {target}:\")\n",
    "    for word, proba in complete_next_word_mle(target).items():\n",
    "        print(f\"- {word}: {proba}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE with Laplace smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation is smoothed via the following formula:\n",
    "\n",
    "$\\^P(w|h) = \\frac{C(h,w) + 1}{C(h) + |V|}$\n",
    "\n",
    "$|V|$ being the size of the vocabulary. More precisely in the code, it's the size of the vocabulary plus 3 tokens: \n",
    "- \\<s> is the start of sentence token\n",
    "- \\</s> is the end of sentence token\n",
    "- \\<UNK> is the token for every word outside of the vocabulary (less than 3 occurence in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mle_laplace (smoothed)\n",
    "\n",
    "best_nexts_key_mle_laplace = {}\n",
    "V = len(vocab) + 3\n",
    "for first_word in bi_count.keys():\n",
    "    for second_word in bi_count[first_word].keys():\n",
    "        previous = bi_count[first_word][second_word]\n",
    "        mle_laplace[first_word][second_word] = (previous + 1) / (h_count[first_word] + V)\n",
    "\n",
    "    key_next = list(mle_laplace[first_word].keys())\n",
    "    key_next = sorted(key_next, key= mle_laplace[first_word].__getitem__)\n",
    "    best_nexts_key_mle_laplace[first_word] = key_next\n",
    "\n",
    "def complete_next_word_mle_laplace(target):\n",
    "    ret = {}\n",
    "    nexts = best_nexts_key_mle_laplace[target]\n",
    "    for i in range(5):\n",
    "        ret[nexts[-i-1]] = round(mle_laplace[target][nexts[-i-1]], 4)\n",
    "    return ret\n",
    "\n",
    "# {'<UNK>': 0.14486107364445644, 'The': 0.05988670083625573, 'A': 0.04963582411653628}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most probable completion after a target word (<s> is the start of a sentence) via the laplace smooth:\n",
      "\n",
      "After the target <s>:\n",
      "- <UNK>: 0.1449\n",
      "- The: 0.0599\n",
      "- A: 0.0496\n",
      "- It: 0.0407\n",
      "- This: 0.0189\n",
      "After the target love:\n",
      "- story: 0.0035\n",
      "- <UNK>: 0.0029\n",
      "- ,: 0.0023\n",
      "- and: 0.0017\n",
      "- with: 0.0017\n"
     ]
    }
   ],
   "source": [
    "# Run example\n",
    "\n",
    "print(\"The 5 most probable completion after a target word (<s> is the start of a sentence) via the laplace smooth:\\n\")\n",
    "for target in [\"<s>\", \"love\"]:\n",
    "    print(f\"After the target {target}:\")\n",
    "    for word, proba in complete_next_word_mle_laplace(target).items():\n",
    "        print(f\"- {word}: {proba}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower it is, the better. Because better model are more predictive and less uniformly random.\n",
    "\n",
    "It can be interpreted as the number of words among which one predicts, if one would predict uniformly at random.\n",
    "\n",
    "$|V| = 10000$ and $PP = 100$ means that the uncertainty is the same as if one would predict only among 100 possibilities with an equal probability of $\\frac{1}{100}$ for each word.\n",
    "\n",
    "The model actually predicts among 10'000 possibilities but with unequal probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set perplexity PP: 167.333\n"
     ]
    }
   ],
   "source": [
    "# Care we use the test_set corpus here\n",
    "# So some words may not be present in the training but do here\n",
    "# Compute using 1/(h_count[bi[0]] + V)))\n",
    "\n",
    "mult_score = 0\n",
    "M = 0\n",
    "for sentence in range(len(test_set)):\n",
    "    for bi in list(bigrams(list(pad_both_ends(test_set[sentence], n=2)))):\n",
    "        if bi[0] == '<s>':\n",
    "            bi = (bi[0], vocab.lookup(bi[1]))\n",
    "        elif bi[1] == '</s>':\n",
    "            bi = (vocab.lookup(bi[0]), bi[1])\n",
    "        else:\n",
    "            bi = (vocab.lookup(bi[0]), vocab.lookup(bi[1]))\n",
    "        mult_score += np.log2(mle_laplace[bi[0]].get(bi[1], \n",
    "                                                     1/(h_count[bi[0]] + V)))\n",
    "        M += 1\n",
    "        \n",
    "LL = mult_score/M\n",
    "PP = 2**(-LL)\n",
    "print(\"Test set perplexity PP: {:.3f}\".format(PP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_neg = 0\n",
    "N_pos = 0\n",
    "N_doc = len(parsed_texts)\n",
    "bag_neg = {}\n",
    "bag_pos = {}\n",
    "for sentence in range(N_doc):\n",
    "    sequence = parsed_texts[sentence]\n",
    "    if int(sequence.label()) < 2:\n",
    "        N_neg += 1\n",
    "        for word in sequence.leaves():\n",
    "            bag_neg[word] = bag_neg.get(word, 0) + 1\n",
    "    else:\n",
    "        N_pos += 1\n",
    "        for word in sequence.leaves():\n",
    "            bag_pos[word] = bag_pos.get(word, 0) + 1\n",
    "\n",
    "count_neg = 0\n",
    "for word in bag_neg.keys():\n",
    "    count_neg += bag_neg[word]\n",
    "count_pos = 0\n",
    "for word in bag_pos.keys():\n",
    "    count_pos += bag_pos[word]\n",
    "\n",
    "#####\n",
    "\n",
    "test_pred = [0]*len(test_set)\n",
    "vocab_len = len(vocab) + 2\n",
    "for sentence in range(len(test_set)):\n",
    "    neg_score = 0\n",
    "    pos_score = 0\n",
    "    for word in test_set[sentence]:\n",
    "        if word in vocab:\n",
    "            neg_score += np.log( (bag_neg.get(word, 0) + 1) / (count_neg + vocab_len) )\n",
    "            pos_score += np.log( (bag_pos.get(word, 0) + 1) / (count_pos + vocab_len) )\n",
    "    \n",
    "    neg_score += np.log((N_neg/N_doc))    # Ajout du prior avec les logs\n",
    "    pos_score += np.log((N_pos/N_doc))\n",
    "    if neg_score > pos_score:\n",
    "        test_pred[sentence] = 0\n",
    "    else:\n",
    "        test_pred[sentence] = 1\n",
    "\n",
    "test_verif = test.parsed_sents()\n",
    "ratio = 0\n",
    "for sentence in range(len(test_set)):\n",
    "    y = int(test_verif[sentence].label())\n",
    "    if (test_pred[sentence] == (y>=2)):\n",
    "        ratio += 1\n",
    "print((ratio/len(test_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Naive Bias\n",
    "N_neg = 0\n",
    "N_pos = 0\n",
    "N_doc = len(parsed_texts)\n",
    "bag_neg = {}\n",
    "bag_pos = {}\n",
    "for sentence in range(N_doc):\n",
    "    sequence = parsed_texts[sentence]\n",
    "    words_seen = []\n",
    "    if int(sequence.label()) < 2:\n",
    "        N_neg += 1\n",
    "        for word in sequence.leaves():\n",
    "            if word not in words_seen:\n",
    "                bag_neg[word] = bag_neg.get(word, 0) + 1\n",
    "                words_seen.append(word)\n",
    "    else:\n",
    "        N_pos += 1\n",
    "        for word in sequence.leaves():\n",
    "            if word not in words_seen:\n",
    "                bag_pos[word] = bag_pos.get(word, 0) + 1\n",
    "                words_seen.append(word)\n",
    "\n",
    "count_neg = 0\n",
    "for word in bag_neg.keys():\n",
    "    count_neg += bag_neg[word]\n",
    "count_pos = 0\n",
    "for word in bag_pos.keys():\n",
    "    count_pos += bag_pos[word]\n",
    "\n",
    "####\n",
    "\n",
    "test_pred = [0]*len(test_set)\n",
    "vocab_len = len(vocab) + 2\n",
    "for sentence in range(len(test_set)):\n",
    "    neg_score = 0\n",
    "    pos_score = 0\n",
    "    words_seen = []\n",
    "    for word in test_set[sentence]:\n",
    "        if word in vocab and word not in words_seen:\n",
    "            neg_score += np.log( (bag_neg.get(word, 0) + 1) / (count_neg + vocab_len) )\n",
    "            pos_score += np.log( (bag_pos.get(word, 0) + 1) / (count_pos + vocab_len) )\n",
    "            words_seen.append(word)\n",
    "    \n",
    "    neg_score += np.log((N_neg/N_doc))\n",
    "    pos_score += np.log((N_pos/N_doc))\n",
    "    if neg_score > pos_score:\n",
    "        test_pred[sentence] = 0\n",
    "    else:\n",
    "        test_pred[sentence] = 1\n",
    "\n",
    "test_verif = test.parsed_sents()\n",
    "ratio = 0\n",
    "for sentence in range(len(test_set)):\n",
    "    y = int(test_verif[sentence].label())\n",
    "    if (test_pred[sentence] == (y>=2)):\n",
    "        ratio += 1\n",
    "print((ratio/len(test_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) negative\n",
    "negate = [\"n't\", \"not\", \"no\", \"never\"]\n",
    "punct = ['.', ',', ':', '?', '!']\n",
    "\n",
    "N_neg = 0\n",
    "N_pos = 0\n",
    "N_doc = len(parsed_texts)\n",
    "bag_neg = {}\n",
    "bag_pos = {}\n",
    "neg_words_seen = set()\n",
    "for sentence in range(N_doc):\n",
    "    sequence = parsed_texts[sentence]\n",
    "    in_neg = False\n",
    "    if int(sequence.label()) < 2:\n",
    "        N_neg += 1\n",
    "        for word in sequence.leaves():\n",
    "            if word in punct: in_neg = False\n",
    "            if in_neg: \n",
    "                if word in vocab:\n",
    "                    neg_words_seen.add(word+\"_NOT\")\n",
    "                word += \"_NOT\"\n",
    "            bag_neg[word] = bag_neg.get(word, 0) + 1\n",
    "            if word in negate: in_neg = True\n",
    "    else:\n",
    "        N_pos += 1\n",
    "        for word in sequence.leaves():\n",
    "            if word in punct: in_neg = False\n",
    "            if in_neg: \n",
    "                if word in vocab:\n",
    "                    neg_words_seen.add(word+\"_NOT\")\n",
    "                word += \"_NOT\"\n",
    "            bag_pos[word] = bag_pos.get(word, 0) + 1\n",
    "            if word in negate: in_neg = True\n",
    "\n",
    "\n",
    "count_neg = 0\n",
    "for word in bag_neg.keys():\n",
    "    count_neg += bag_neg[word]\n",
    "count_pos = 0\n",
    "for word in bag_pos.keys():\n",
    "    count_pos += bag_pos[word]\n",
    "    \n",
    "######\n",
    "\n",
    "test_pred = [0]*len(test_set)\n",
    "vocab_len = len(vocab) + 2\n",
    "for sentence in range(len(test_set)):\n",
    "    neg_score = 0\n",
    "    pos_score = 0\n",
    "    in_neg = False\n",
    "    for word in test_set[sentence]:\n",
    "        if word in punct: in_neg = False\n",
    "        if in_neg:\n",
    "            word += \"_NOT\"\n",
    "        if word in vocab or word in neg_words_seen:\n",
    "            neg_score += np.log( (bag_neg.get(word, 0) + 1) / (count_neg + vocab_len) )\n",
    "            pos_score += np.log( (bag_pos.get(word, 0) + 1) / (count_pos + vocab_len) )\n",
    "        if word in negate: in_neg = True\n",
    "    \n",
    "    neg_score += np.log((N_neg/N_doc))\n",
    "    pos_score += np.log((N_pos/N_doc))\n",
    "    if neg_score > pos_score:\n",
    "        test_pred[sentence] = 0\n",
    "    else:\n",
    "        test_pred[sentence] = 1\n",
    "\n",
    "test_verif = test.parsed_sents()\n",
    "ratio = 0\n",
    "for sentence in range(len(test_set)):\n",
    "    y = int(test_verif[sentence].label())\n",
    "    if (test_pred[sentence] == (y>=2)):\n",
    "        ratio += 1\n",
    "print((ratio/len(test_set))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
