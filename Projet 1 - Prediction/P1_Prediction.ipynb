{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction: \n",
    "This project is about completion (prediction) and categorization of words.\n",
    "\n",
    "It involves the use of:\n",
    "\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import BracketParseCorpusReader\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import bigrams\n",
    "import numpy as np\n",
    "\n",
    "corpus = BracketParseCorpusReader(root=\"corpora\", fileids=[\"p1_train.txt\"])\n",
    "words = corpus.words()\n",
    "sentences = corpus.sents()\n",
    "parsed_texts = corpus.parsed_sents()\n",
    "test = BracketParseCorpusReader(root=\"ressources\", fileids=[\"p1_test.txt\"])\n",
    "test_set = test.sents()\n",
    "vocab = Vocabulary(words, unk_cutoff=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little checks over the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our vocabulary, we have put all the words of the corpus that appear at least 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 last words of the vocabulary with their number of appearence in the corpus:\n",
      "  [('wry', 6), ('year', 32), ('years', 8), ('yes', 3), ('yet', 11), ('you', 166), ('young', 12), ('your', 31), ('yourself', 5), ('zippy', 3)]\n"
     ]
    }
   ],
   "source": [
    "word_num = []\n",
    "for word in sorted(vocab)[-10:]:\n",
    "    word_num.append((word, vocab[word]))\n",
    "print(f\"The 10 last words of the vocabulary with their number of appearence in the corpus:\\n  {word_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unkown words: 7352\n",
      "Out of vocabulary (OOV) rate: 19.295%\n"
     ]
    }
   ],
   "source": [
    "count_unk = 0\n",
    "for word in words:\n",
    "    if word not in vocab:\n",
    "        count_unk += 1\n",
    "oov = (count_unk/len(words))*100\n",
    "\n",
    "print(f\"The number of unkown words: {count_unk}\")\n",
    "print(f\"Out of vocabulary (OOV) rate: {round(oov, 3)}%\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the bigrams of the 10th sentence:\n",
      "  [('<s>', 'It'), ('It', \"'s\"), (\"'s\", 'a'), ('a', 'coming-of-age'), ('coming-of-age', 'story'), ('story', 'we'), ('we', \"'ve\"), (\"'ve\", 'all'), ('all', 'seen'), ('seen', 'bits'), ('bits', 'of'), ('of', 'in'), ('in', 'other'), ('other', 'films'), ('films', '--'), ('--', 'but'), ('but', 'it'), ('it', \"'s\"), (\"'s\", 'rarely'), ('rarely', 'been'), ('been', 'told'), ('told', 'with'), ('with', 'such'), ('such', 'affecting'), ('affecting', 'grace'), ('grace', 'and'), ('and', 'cultural'), ('cultural', 'specificity'), ('specificity', '.'), ('.', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "sentence_10_bi = list(bigrams(list(pad_both_ends(sentences[9], n=2))))\n",
    "print(f\"All the bigrams of the 10th sentence:\\n  {sentence_10_bi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 0.268, 'The': 0.1105, 'A': 0.0915}\n"
     ]
    }
   ],
   "source": [
    "mle_pers = {}\n",
    "for sentence in range(len(sentences)):\n",
    "    for bi in list(bigrams(list(pad_both_ends(sentences[sentence], n=2)))):\n",
    "        if bi[0] == '<s>':\n",
    "            bi = (bi[0], vocab.lookup(bi[1]))\n",
    "        elif bi[1] == '</s>':\n",
    "            bi = (vocab.lookup(bi[0]), bi[1])\n",
    "        else:\n",
    "            bi = (vocab.lookup(bi[0]), vocab.lookup(bi[1]))\n",
    "        if mle_pers.get(bi[0], -1) == -1:\n",
    "            mle_pers[bi[0]] = {}\n",
    "        count_bi = mle_pers[bi[0]].get(bi[1], 0) # 0 ou 1 de base ? C'était 0 ici mais la case d'après c'était le même code avec 1 à la place\n",
    "        mle_pers[bi[0]][bi[1]] = count_bi+1\n",
    "\n",
    "count_nexts = {}\n",
    "key_nexts = {}\n",
    "for first_word in mle_pers.keys():    \n",
    "    count_next = 0\n",
    "    for i in mle_pers[first_word]:\n",
    "        count_next += mle_pers[first_word][i]\n",
    "    count_nexts[first_word] = count_next\n",
    "    for second_word in mle_pers[first_word].keys():\n",
    "        previous = mle_pers[first_word][second_word]\n",
    "        mle_pers[first_word][second_word] = previous/count_nexts[first_word]\n",
    "        \n",
    "    key_next = list(mle_pers[first_word].keys())\n",
    "    key_next = sorted(key_next, key= mle_pers[first_word].__getitem__)\n",
    "    key_nexts[first_word] = key_next   \n",
    "\n",
    "ret_dict_mle = {}\n",
    "ret_dict_mle[key_nexts[\"<s>\"][-1]] = mle_pers[\"<s>\"][key_nexts[\"<s>\"][-1]]\n",
    "ret_dict_mle[key_nexts[\"<s>\"][-2]] = mle_pers[\"<s>\"][key_nexts[\"<s>\"][-2]]\n",
    "ret_dict_mle[key_nexts[\"<s>\"][-3]] = mle_pers[\"<s>\"][key_nexts[\"<s>\"][-3]]\n",
    "print(\"The completion to start a sentence:\")\n",
    "print(ret_dict_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_pers = {}\n",
    "        \n",
    "for sentence in range(len(sentences)):\n",
    "    for bi in list(bigrams(list(pad_both_ends(sentences[sentence], n=2)))):\n",
    "        if bi[0] == '<s>':\n",
    "            bi = (bi[0], vocab.lookup(bi[1]))\n",
    "        elif bi[1] == '</s>':\n",
    "            bi = (vocab.lookup(bi[0]), bi[1])\n",
    "        else:\n",
    "            bi = (vocab.lookup(bi[0]), vocab.lookup(bi[1]))\n",
    "        if mle_pers.get(bi[0], -1) == -1:\n",
    "            mle_pers[bi[0]] = {}\n",
    "        count_bi = mle_pers[bi[0]].get(bi[1], 1)\n",
    "        mle_pers[bi[0]][bi[1]] = count_bi+1\n",
    "\n",
    "key_nexts = {}\n",
    "len_vocab = len(vocab)\n",
    "for first_word in mle_pers.keys():\n",
    "    for second_word in mle_pers[first_word].keys():\n",
    "        previous = mle_pers[first_word][second_word]\n",
    "        mle_pers[first_word][second_word] = previous/(count_nexts[first_word] + len_vocab + 3)\n",
    "    key_next = list(mle_pers[first_word].keys())\n",
    "    key_next = sorted(key_next, key= mle_pers[first_word].__getitem__)\n",
    "    key_nexts[first_word] = key_next\n",
    "\n",
    "ret_dict_mle = {}\n",
    "ret_dict_mle[key_nexts[\"<s>\"][-1]] = mle_pers[\"<s>\"][key_nexts[\"<s>\"][-1]]\n",
    "ret_dict_mle[key_nexts[\"<s>\"][-2]] = mle_pers[\"<s>\"][key_nexts[\"<s>\"][-2]]\n",
    "ret_dict_mle[key_nexts[\"<s>\"][-3]] = mle_pers[\"<s>\"][key_nexts[\"<s>\"][-3]]\n",
    "print(ret_dict_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set perplexity\n",
    "\n",
    "The lower it is, the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_score = 0\n",
    "M = 0\n",
    "for sentence in range(len(test_set)):\n",
    "    for bi in list(bigrams(list(pad_both_ends(test_set[sentence], n=2)))):\n",
    "        if bi[0] == '<s>':\n",
    "            bi = (bi[0], vocab.lookup(bi[1]))\n",
    "        elif bi[1] == '</s>':\n",
    "            bi = (vocab.lookup(bi[0]), bi[1])\n",
    "        else:\n",
    "            bi = (vocab.lookup(bi[0]), vocab.lookup(bi[1]))\n",
    "        mult_score += np.log2(mle_pers[bi[0]].get(bi[1], 1/(count_nexts[bi[0]] + len_vocab + 3)))\n",
    "        M += 1\n",
    "        \n",
    "LL = mult_score/M\n",
    "PP = 2**(-LL)\n",
    "print(\"{:.3f}\".format(PP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_neg = 0\n",
    "N_pos = 0\n",
    "N_doc = len(parsed_texts)\n",
    "bag_neg = {}\n",
    "bag_pos = {}\n",
    "for sentence in range(N_doc):\n",
    "    sequence = parsed_texts[sentence]\n",
    "    if int(sequence.label()) < 2:\n",
    "        N_neg += 1\n",
    "        for word in sequence.leaves():\n",
    "            bag_neg[word] = bag_neg.get(word, 0) + 1\n",
    "    else:\n",
    "        N_pos += 1\n",
    "        for word in sequence.leaves():\n",
    "            bag_pos[word] = bag_pos.get(word, 0) + 1\n",
    "\n",
    "count_neg = 0\n",
    "for word in bag_neg.keys():\n",
    "    count_neg += bag_neg[word]\n",
    "count_pos = 0\n",
    "for word in bag_pos.keys():\n",
    "    count_pos += bag_pos[word]\n",
    "\n",
    "#####\n",
    "\n",
    "test_pred = [0]*len(test_set)\n",
    "vocab_len = len(vocab) + 2\n",
    "for sentence in range(len(test_set)):\n",
    "    neg_score = 0\n",
    "    pos_score = 0\n",
    "    for word in test_set[sentence]:\n",
    "        if word in vocab:\n",
    "            neg_score += np.log( (bag_neg.get(word, 0) + 1) / (count_neg + vocab_len) )\n",
    "            pos_score += np.log( (bag_pos.get(word, 0) + 1) / (count_pos + vocab_len) )\n",
    "    \n",
    "    neg_score += np.log((N_neg/N_doc))    # Ajout du prior avec les logs\n",
    "    pos_score += np.log((N_pos/N_doc))\n",
    "    if neg_score > pos_score:\n",
    "        test_pred[sentence] = 0\n",
    "    else:\n",
    "        test_pred[sentence] = 1\n",
    "\n",
    "test_verif = test.parsed_sents()\n",
    "ratio = 0\n",
    "for sentence in range(len(test_set)):\n",
    "    y = int(test_verif[sentence].label())\n",
    "    if (test_pred[sentence] == (y>=2)):\n",
    "        ratio += 1\n",
    "print((ratio/len(test_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Naive Bias\n",
    "N_neg = 0\n",
    "N_pos = 0\n",
    "N_doc = len(parsed_texts)\n",
    "bag_neg = {}\n",
    "bag_pos = {}\n",
    "for sentence in range(N_doc):\n",
    "    sequence = parsed_texts[sentence]\n",
    "    words_seen = []\n",
    "    if int(sequence.label()) < 2:\n",
    "        N_neg += 1\n",
    "        for word in sequence.leaves():\n",
    "            if word not in words_seen:\n",
    "                bag_neg[word] = bag_neg.get(word, 0) + 1\n",
    "                words_seen.append(word)\n",
    "    else:\n",
    "        N_pos += 1\n",
    "        for word in sequence.leaves():\n",
    "            if word not in words_seen:\n",
    "                bag_pos[word] = bag_pos.get(word, 0) + 1\n",
    "                words_seen.append(word)\n",
    "\n",
    "count_neg = 0\n",
    "for word in bag_neg.keys():\n",
    "    count_neg += bag_neg[word]\n",
    "count_pos = 0\n",
    "for word in bag_pos.keys():\n",
    "    count_pos += bag_pos[word]\n",
    "\n",
    "####\n",
    "\n",
    "test_pred = [0]*len(test_set)\n",
    "vocab_len = len(vocab) + 2\n",
    "for sentence in range(len(test_set)):\n",
    "    neg_score = 0\n",
    "    pos_score = 0\n",
    "    words_seen = []\n",
    "    for word in test_set[sentence]:\n",
    "        if word in vocab and word not in words_seen:\n",
    "            neg_score += np.log( (bag_neg.get(word, 0) + 1) / (count_neg + vocab_len) )\n",
    "            pos_score += np.log( (bag_pos.get(word, 0) + 1) / (count_pos + vocab_len) )\n",
    "            words_seen.append(word)\n",
    "    \n",
    "    neg_score += np.log((N_neg/N_doc))\n",
    "    pos_score += np.log((N_pos/N_doc))\n",
    "    if neg_score > pos_score:\n",
    "        test_pred[sentence] = 0\n",
    "    else:\n",
    "        test_pred[sentence] = 1\n",
    "\n",
    "test_verif = test.parsed_sents()\n",
    "ratio = 0\n",
    "for sentence in range(len(test_set)):\n",
    "    y = int(test_verif[sentence].label())\n",
    "    if (test_pred[sentence] == (y>=2)):\n",
    "        ratio += 1\n",
    "print((ratio/len(test_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) negative\n",
    "negate = [\"n't\", \"not\", \"no\", \"never\"]\n",
    "punct = ['.', ',', ':', '?', '!']\n",
    "\n",
    "N_neg = 0\n",
    "N_pos = 0\n",
    "N_doc = len(parsed_texts)\n",
    "bag_neg = {}\n",
    "bag_pos = {}\n",
    "neg_words_seen = set()\n",
    "for sentence in range(N_doc):\n",
    "    sequence = parsed_texts[sentence]\n",
    "    in_neg = False\n",
    "    if int(sequence.label()) < 2:\n",
    "        N_neg += 1\n",
    "        for word in sequence.leaves():\n",
    "            if word in punct: in_neg = False\n",
    "            if in_neg: \n",
    "                if word in vocab:\n",
    "                    neg_words_seen.add(word+\"_NOT\")\n",
    "                word += \"_NOT\"\n",
    "            bag_neg[word] = bag_neg.get(word, 0) + 1\n",
    "            if word in negate: in_neg = True\n",
    "    else:\n",
    "        N_pos += 1\n",
    "        for word in sequence.leaves():\n",
    "            if word in punct: in_neg = False\n",
    "            if in_neg: \n",
    "                if word in vocab:\n",
    "                    neg_words_seen.add(word+\"_NOT\")\n",
    "                word += \"_NOT\"\n",
    "            bag_pos[word] = bag_pos.get(word, 0) + 1\n",
    "            if word in negate: in_neg = True\n",
    "\n",
    "\n",
    "count_neg = 0\n",
    "for word in bag_neg.keys():\n",
    "    count_neg += bag_neg[word]\n",
    "count_pos = 0\n",
    "for word in bag_pos.keys():\n",
    "    count_pos += bag_pos[word]\n",
    "    \n",
    "######\n",
    "\n",
    "test_pred = [0]*len(test_set)\n",
    "vocab_len = len(vocab) + 2\n",
    "for sentence in range(len(test_set)):\n",
    "    neg_score = 0\n",
    "    pos_score = 0\n",
    "    in_neg = False\n",
    "    for word in test_set[sentence]:\n",
    "        if word in punct: in_neg = False\n",
    "        if in_neg:\n",
    "            word += \"_NOT\"\n",
    "        if word in vocab or word in neg_words_seen:\n",
    "            neg_score += np.log( (bag_neg.get(word, 0) + 1) / (count_neg + vocab_len) )\n",
    "            pos_score += np.log( (bag_pos.get(word, 0) + 1) / (count_pos + vocab_len) )\n",
    "        if word in negate: in_neg = True\n",
    "    \n",
    "    neg_score += np.log((N_neg/N_doc))\n",
    "    pos_score += np.log((N_pos/N_doc))\n",
    "    if neg_score > pos_score:\n",
    "        test_pred[sentence] = 0\n",
    "    else:\n",
    "        test_pred[sentence] = 1\n",
    "\n",
    "test_verif = test.parsed_sents()\n",
    "ratio = 0\n",
    "for sentence in range(len(test_set)):\n",
    "    y = int(test_verif[sentence].label())\n",
    "    if (test_pred[sentence] == (y>=2)):\n",
    "        ratio += 1\n",
    "print((ratio/len(test_set))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
