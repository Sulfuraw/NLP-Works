{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'noticed', 'beams', 'boars', 'enables', 'velvet'}\n",
      "{'caretaker', 'existence', 'scion', 'sketch', 'desmonds'}\n",
      "{'crevice', 'trench', 'grazier', 'miles', 'ordeal'}\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Vector Semantics (TF-IDF)\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.lm import Vocabulary\n",
    "import numpy as np\n",
    "\n",
    "# above - maid - distance\n",
    "corpus = PlaintextCorpusReader('/home/thomas/Bureau/LINFO2263/Project_2/corpora', 'p2_sherlock\\.txt').words() \n",
    "vocab = Vocabulary(corpus, unk_cutoff=1)\n",
    "punct = [\"\\\"\", \"'\", \",\", \".\", \":\", \"?\", \"!\", \".\\\"\", \"?\\\"\", \"!\\\"\", \"-\", \",\\\"\", \"--\"]\n",
    "\n",
    "# Give tf-idf for a target words and store non-zeros values\n",
    "def tf_idf_one(target):\n",
    "    closest = {}\n",
    "    for j in dictt[target]:\n",
    "        tf_ij = np.log10( dictt[target].get(j, 0) + 1 )\n",
    "        idf_j = np.log10( N / df.get(j, 0) )\n",
    "        w_ij = tf_ij*idf_j\n",
    "        closest[j] =  w_ij\n",
    "\n",
    "    return closest\n",
    "\n",
    "# Give the cosine_similarity between two words\n",
    "def cosine_similarity(word_1, word_2):\n",
    "    sum_1 = 0\n",
    "    sum_2 = 0\n",
    "    sum_3 = 0\n",
    "    for word, value in tf_idf[word_1].items():\n",
    "        if word in tf_idf[word_2].keys():\n",
    "            sum_1 += value*tf_idf[word_2][word]\n",
    "        sum_2 += value**2\n",
    "    for word_2, value_2 in tf_idf[word_2].items():\n",
    "        sum_3 += value_2**2\n",
    "    \n",
    "    sum_2 = np.sqrt(sum_2)\n",
    "    sum_3 = np.sqrt(sum_3)\n",
    "    return sum_1 / (sum_2 * sum_3)\n",
    "\n",
    "# Give the top 5 closest word of the target\n",
    "def closest_words(target):\n",
    "    closest = []\n",
    "    for word in set(words):\n",
    "        closest.append( (word, cosine_similarity(target, word)) )\n",
    "    closest = sorted(closest, key=lambda entry: entry[1])\n",
    "    \n",
    "    ret_set = set()\n",
    "    for word, value in closest[-6:-1]:\n",
    "        ret_set.add(word)\n",
    "    return ret_set\n",
    "\n",
    "# Preprocessing\n",
    "words = []\n",
    "test = {}\n",
    "for i in range(len(corpus)):\n",
    "    if corpus[i] not in punct:\n",
    "        words.append(corpus[i].lower())\n",
    "        test[corpus[i].lower()] = test.get(corpus[i].lower(), 0)+1\n",
    "\n",
    "# Compute C(w_i, c_j)\n",
    "dictt = {}\n",
    "df = {}\n",
    "N = 0\n",
    "for i in range(len(words)):\n",
    "    if dictt.get(words[i], 0) == 0:\n",
    "        dictt[words[i]] = {}\n",
    "    N+=1\n",
    "    for j in [i-2, i-1, i+1, i+2]:\n",
    "        if j not in [-2, -1, len(words), len(words)+1]:\n",
    "            dictt[words[i]][words[j]] = dictt[words[i]].get(words[j], 0)+1\n",
    "            df[words[j]] = df.get(words[j], 0)+1\n",
    "\n",
    "# Compute tf-idf weighted 'matrix'\n",
    "tf_idf = {}\n",
    "for i in set(words):\n",
    "    tf_idf[i] = tf_idf_one(i)\n",
    "    \n",
    "\n",
    "print(closest_words(\"above\"))\n",
    "print(closest_words(\"maid\"))\n",
    "print(closest_words(\"distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Vector Semantics (PPMI)\n",
    "# Use PPMI technique here\n",
    "\n",
    "# 5 closest words to \"above\" with epsilon of 0,0001\n",
    "\n",
    "# 5 closest words to \"maid\" same epsilon\n",
    "\n",
    "# 5 closest words to \"distance\" same epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('shone', 0.8492485284805298), ('moon', 0.7147756218910217), ('swung', 0.7015173435211182), ('escape', 0.6910266876220703), ('jaws', 0.6594571471214294)]\n",
      "[('oldmore', 0.6325182318687439), ('swung', 0.6313623785972595), ('lodge', 0.6225625276565552), ('offices', 0.6021811962127686), ('hounds', 0.5777137875556946)]\n",
      "[('divided', 0.6811988353729248), ('pace', 0.596568763256073), ('later', 0.5763490200042725), ('top', 0.5754624009132385), ('bit', 0.5565328001976013)]\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Vector Semantics (Word2Vec)\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.lm import Vocabulary\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# above - maid - distance\n",
    "corpus = PlaintextCorpusReader('/home/thomas/Bureau/LINFO2263/Project_2/corpora', 'p2_sherlock\\.txt').words() \n",
    "vocab = Vocabulary(corpus, unk_cutoff=1)\n",
    "punct = [\"\\\"\", \"'\", \",\", \".\", \":\", \"?\", \"!\", \".\\\"\", \"?\\\"\", \"!\\\"\", \"-\", \",\\\"\", \"--\"]\n",
    "\n",
    "# Preprocessing\n",
    "words = []\n",
    "for i in range(len(corpus)):\n",
    "    if corpus[i] not in punct:\n",
    "        words.append(corpus[i].lower())\n",
    "\n",
    "model = Word2Vec(sentences=[words], vector_size=100, window=2, min_count=2, sg=1, negative=10, epochs=300)\n",
    "most_sim_1 = model.wv.most_similar('above', topn=5)\n",
    "most_sim_2 = model.wv.most_similar('maid', topn=5)\n",
    "most_sim_3 = model.wv.most_similar('distance', topn=5)\n",
    "print(most_sim_1)\n",
    "print(most_sim_2)\n",
    "print(most_sim_3)\n",
    "\n",
    "# import gensim.downloader\n",
    "# glove_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "# most_sim = glove_vectors.most_similar(\"investigation\", topn=5)\n",
    "# print(most_sim)\n",
    "# [('investigations', 0.833749532699585), ('probe', 0.7943025827407837), ('inquiry', 0.7801670432090759), ('investgation', 0.6887422204017639), ('investigaton', 0.6771849989891052)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
