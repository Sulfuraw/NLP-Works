{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity: Vector Semantics\n",
    "\n",
    "This project is about the similarity of words given a corpus that gives us a context for each word. From that similarity we can derive semantic meaning of words and further relationships between them.\n",
    "\n",
    "*The meaning of a word is its use in the language* -  Wittgenstein, 1953  \n",
    "Language use of a word can be characterized by counting occurence of other words around it.\n",
    "\n",
    "It involves the use of:\n",
    "\n",
    "- TF-IDF: The weighted frequency of words appearing together\n",
    "- PPMI: The measure of how much two words co-occur more than expected by chance\n",
    "- Word2Vec: Use a shallow neural network to predict words in the context of other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import downloader\n",
    "\n",
    "corpus = PlaintextCorpusReader(os.getcwd()+'/corpora', 'p2_sherlock\\.txt').words()\n",
    "punct = [\"\\\"\", \"'\", \",\", \".\", \":\", \"?\", \"!\", \".\\\"\", \"?\\\"\", \"!\\\"\", \"-\", \",\\\"\", \"--\"]\n",
    "\n",
    "# Preprocessing\n",
    "words = []\n",
    "for i in range(len(corpus)):\n",
    "    if corpus[i] not in punct:\n",
    "        words.append(corpus[i].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C(w_i, c_j)$:\n",
    "- The number of times context word cj occurs in the local contexts of target word wi\n",
    "- Forms the co-occurence matrix\n",
    "\n",
    "$tf_{i,j} = log_{10} (C(w_i, c_j) + 1)$:\n",
    "- Term frequency\n",
    "- Smoothed co-occurence frequency on a log scale\n",
    "\n",
    "$idf_j = log_{10} \\frac{N}{df_j}$:\n",
    "- $df_j$ the number of contextual windows of any target word where this context word $c_j$ occurs\n",
    "- N the total number of contextual windows for all target words\n",
    "- Words like \"the, a, it, etc...\" have a high document frequency and so, a low inverse document frequency\n",
    "\n",
    "TF-IDF: $w_{i,j} = tf_{i,j} \\times idf_j$\n",
    "- Weighted frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute C(w_i, c_j)  (dictionnary)\n",
    "# Via N-Gram 5 (2 before and 2 after the word). Could use nltk.ngrams(words, 5) to have the list of 5-gram\n",
    "# Accessing the contextual windows\n",
    "dictionary = {}\n",
    "# Compute df_j & N\n",
    "df = {}\n",
    "N = 0\n",
    "for i in range(len(words)):\n",
    "    if dictionary.get(words[i], 0) == 0:\n",
    "        dictionary[words[i]] = {}\n",
    "    N+=1\n",
    "    for j in [i-2, i-1, i+1, i+2]:\n",
    "        if j not in [-2, -1, len(words), len(words)+1]:\n",
    "            dictionary[words[i]][words[j]] = dictionary[words[i]].get(words[j], 0)+1\n",
    "            df[words[j]] = df.get(words[j], 0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sir': 2.6499717475814424,\n",
       " 'henry': 2.8043613034991757,\n",
       " 'three': 0.8045531714532216,\n",
       " 'broken': 1.4464836063347186,\n",
       " 'threads': 1.0472375255259798,\n",
       " 'hall': 3.542267416272485,\n",
       " 'the': 0.8359804946545215,\n",
       " 'charles': 2.7127583202582395,\n",
       " 'whose': 1.5380362693763716,\n",
       " 'sudden': 0.9857912880321348,\n",
       " 'was': 1.2143324830523035,\n",
       " 'written': 1.0032486035793737,\n",
       " 'and': 1.2348030417274845,\n",
       " 'in': 1.0291797662182496,\n",
       " 'family': 1.2451032785465193,\n",
       " 'but': 1.100649466082976,\n",
       " 'from': 0.8198972231791053,\n",
       " 'hugo': 2.513066165257388,\n",
       " 'as': 1.325302482073515,\n",
       " 'manor': 1.2576484629005042,\n",
       " 'of': 1.1147394838432119,\n",
       " 'held': 0.8596207943399203,\n",
       " 'near': 1.4062523195504442,\n",
       " 'estate': 0.9703928444215979,\n",
       " 'by': 0.8843610913923687,\n",
       " 'when': 0.964969018249066,\n",
       " 'he': 0.8489006436580457,\n",
       " 'for': 1.2088857214534718,\n",
       " 'passed': 0.8087038586721457,\n",
       " 'me': 0.7942161180589478,\n",
       " 'lying': 1.0472375255259798,\n",
       " 'on': 0.8602827422951178,\n",
       " 'which': 0.743203050701596,\n",
       " 'to': 1.1322674532909247,\n",
       " 'his': 0.9350203303433943,\n",
       " 'occurred': 0.9223181003856566,\n",
       " 'name': 1.2045428238961624,\n",
       " 'resided': 1.2576484629005042,\n",
       " 'at': 1.2580498716416133,\n",
       " 'residence': 1.1670294046110479,\n",
       " 'servants': 1.114020653661051,\n",
       " 'consisted': 1.2576484629005042,\n",
       " 'alley': 0.8951722297426782,\n",
       " 'tenant': 1.0472375255259798,\n",
       " 'it': 0.7061271898686518,\n",
       " 'mr': 1.134822032163642,\n",
       " 'if': 1.1209438396993987,\n",
       " 's': 1.408813145637887,\n",
       " 'younger': 1.706070034307995,\n",
       " 'i': 0.7792695083238416,\n",
       " 'must': 0.6737949906945135,\n",
       " 'motive': 1.0472375255259798,\n",
       " 'that': 1.1657263677215248,\n",
       " 'with': 1.1932009207302485,\n",
       " 'reach': 0.9566184672365232,\n",
       " 'within': 0.8174227999339668,\n",
       " 'this': 0.7855032555913286,\n",
       " 'demon': 1.114020653661051,\n",
       " 'who': 1.3991467827228656,\n",
       " 'arrives': 1.2576484629005042,\n",
       " 'rodger': 2.0944750510519596,\n",
       " 'youngest': 1.2576484629005042,\n",
       " 'old': 0.7060083128543391,\n",
       " 'masterful': 1.114020653661051,\n",
       " 'strain': 1.2576484629005042,\n",
       " 'every': 1.5783094556853696,\n",
       " 'goes': 0.9441580274028221,\n",
       " 'a': 1.012968668776438,\n",
       " 'is': 1.0748163015089056,\n",
       " 'then': 0.6008480970820044,\n",
       " 'you': 0.8776347650882971,\n",
       " 'death': 0.711907163011132,\n",
       " 'our': 0.890338749996956,\n",
       " 'breakfast': 0.9441580274028221,\n",
       " 'said': 1.6234004493861007,\n",
       " 'dr': 1.0193338134686682,\n",
       " 'northumberland': 1.562442225068542,\n",
       " 'hotel': 1.3182117936081834,\n",
       " 'perhaps': 0.8220104870004605,\n",
       " 'turned': 0.7629199108239091,\n",
       " 'someone': 0.8951722297426782,\n",
       " 'cut': 0.912629545289917,\n",
       " 'listened': 1.0472375255259798,\n",
       " 'go': 1.1415068418181165,\n",
       " 'why': 0.7139341131637652,\n",
       " 'mortimer': 0.6693628487576166,\n",
       " 'were': 1.1209438396993987,\n",
       " 'still': 0.7599819070470731,\n",
       " 'heard': 0.711907163011132,\n",
       " 'has': 0.5854496534714674,\n",
       " 'been': 0.8738307776018008,\n",
       " 'had': 0.8872360890829343,\n",
       " 'followed': 0.8316990420962,\n",
       " 'home': 0.8535389691133657,\n",
       " 'we': 0.7201820161394921,\n",
       " 'upstairs': 1.0472375255259798,\n",
       " 'one': 0.5373749094355261,\n",
       " 'himself': 0.7820862136609175,\n",
       " 'holmes': 0.5689515180595597,\n",
       " 'asked': 0.7753803506576101,\n",
       " 'what': 0.8853454691492445,\n",
       " 'barrymore': 0.6967323903978465,\n",
       " 'so': 0.5389725138295238,\n",
       " 'time': 0.6753064025106749,\n",
       " 'since': 0.751544420503225,\n",
       " 'answer': 0.8368265881514549,\n",
       " 'seized': 1.1670294046110479,\n",
       " 'down': 0.6407723704215476,\n",
       " 'depart': 1.2576484629005042,\n",
       " 'gave': 1.2955844851298295,\n",
       " 'inch': 1.0764103463215913,\n",
       " 'second': 0.8872464294332522,\n",
       " 'consulted': 1.2576484629005042,\n",
       " 'chapter': 1.4188144158290885,\n",
       " 'vi': 1.2576484629005042,\n",
       " 'between': 0.8005302181633689,\n",
       " 'young': 1.3624667238689068,\n",
       " 'upon': 0.8019874461514841,\n",
       " 'park': 1.1670294046110479,\n",
       " 'climate': 1.2576484629005042,\n",
       " 'stared': 0.932782537082138,\n",
       " 'eagerly': 1.0472375255259798,\n",
       " 'last': 0.6949536464149125,\n",
       " 'saw': 0.7202106304084369,\n",
       " 'dream': 1.114020653661051,\n",
       " 'sat': 0.8535389691133657,\n",
       " 'turn': 0.9036097162865264,\n",
       " 'an': 0.5636685501520341,\n",
       " 'sky': 0.9857912880321348,\n",
       " 'even': 0.8174227999339668,\n",
       " 'fell': 0.932782537082138,\n",
       " 'silent': 0.9223181003856566,\n",
       " 'whip': 1.2576484629005042,\n",
       " 'heads': 1.0234015953715945,\n",
       " 'shuddered': 1.2576484629005042,\n",
       " 'welcome': 1.0764103463215913,\n",
       " 'minds': 1.0234015953715945,\n",
       " 'wing': 1.114020653661051,\n",
       " 'experience': 0.9857912880321348,\n",
       " 'persecuting': 1.2576484629005042,\n",
       " 'no': 0.8371435409199269,\n",
       " 'way': 0.711907163011132,\n",
       " 'possibly': 0.8951722297426782,\n",
       " 'do': 0.5824267288303243,\n",
       " 'october': 1.0764103463215913,\n",
       " 'call': 0.8951722297426782,\n",
       " 'moor': 0.5909140289263104,\n",
       " 'oct': 1.2576484629005042,\n",
       " 'direction': 0.8596207943399203,\n",
       " 'towers': 1.706070034307995,\n",
       " 'rose': 0.8797737861321414,\n",
       " 'very': 0.5716752762863927,\n",
       " 'made': 1.159227987855812,\n",
       " 'some': 0.6106690427369098,\n",
       " 'be': 0.4995002224993877,\n",
       " 'there': 0.5064147617896242,\n",
       " 'off': 0.7820862136609175,\n",
       " 'leaving': 1.0764103463215913,\n",
       " 'gates': 1.114020653661051,\n",
       " 'are': 0.5649694132830855,\n",
       " 'rear': 1.2576484629005042,\n",
       " 'admiral': 1.2576484629005042,\n",
       " 'served': 1.0472375255259798,\n",
       " 'william': 1.2576484629005042,\n",
       " 'my': 0.45078707491930353,\n",
       " 'baronet': 0.751544420503225,\n",
       " 'mystery': 0.9703928444215979,\n",
       " 'recollection': 1.0764103463215913,\n",
       " 'indeed': 0.8129906579970699,\n",
       " 'perceive': 1.0764103463215913,\n",
       " 'stapleton': 0.6650760494326924,\n",
       " 'single': 0.9566184672365232,\n",
       " 'affair': 0.9441580274028221}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give tf-idf for a target words: Would correspond to wj, because i is fixed to the target\n",
    "def tf_idf_one(target):\n",
    "    closest = {}\n",
    "    for j in dictionary[target]:\n",
    "        tf_ij = np.log10( dictionary[target].get(j, 0) + 1 )\n",
    "        idf_j = np.log10( N / df.get(j, 0) )\n",
    "        w_ij = tf_ij*idf_j\n",
    "        closest[j] =  w_ij\n",
    "    return closest\n",
    "\n",
    "# Compute tf-idf weighted matrix: w\n",
    "tf_idf = {}\n",
    "for i in set(words):\n",
    "    tf_idf[i] = tf_idf_one(i)\n",
    "\n",
    "tf_idf[\"baskerville\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine simiarity between two words:\n",
    "\n",
    "$cos(v, w) = \\frac{\\sum_{i=1}^d v_i w_i}{\\sqrt{\\sum_{i=1}^d v_i^2} \\sqrt{\\sum_{i=1}^d w_i^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(word_1, word_2):\n",
    "    top_sum = 0\n",
    "    bot_left_sum = 0\n",
    "    bot_right_sum = 0\n",
    "    for word, value in tf_idf[word_1].items():\n",
    "        if word in tf_idf[word_2].keys():\n",
    "            top_sum += value*tf_idf[word_2][word]\n",
    "        bot_left_sum += value**2\n",
    "    for word_2, value_2 in tf_idf[word_2].items():\n",
    "        bot_right_sum += value_2**2\n",
    "    \n",
    "    bot_left_sum = np.sqrt(bot_left_sum)\n",
    "    bot_right_sum = np.sqrt(bot_right_sum)\n",
    "    return top_sum / (bot_left_sum * bot_right_sum)\n",
    "\n",
    "# Give the top 5 closest word of the target\n",
    "# Words used in the same context\n",
    "def closest_words(target):\n",
    "    closest = []\n",
    "    for word in set(words):\n",
    "        closest.append( (word, cosine_similarity(target, word)) )\n",
    "    closest = sorted(closest, key=lambda entry: -entry[1])\n",
    "\n",
    "    return closest[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words to above:\n",
      "- beams: 0.329\n",
      "- noticed: 0.253\n",
      "- velvet: 0.218\n",
      "- enables: 0.212\n",
      "- boars: 0.212\n",
      "Closest words to maid:\n",
      "- scion: 0.277\n",
      "- caretaker: 0.233\n",
      "- existence: 0.229\n",
      "- sketch: 0.227\n",
      "- desmonds: 0.217\n",
      "Closest words to distance:\n",
      "- grazier: 0.401\n",
      "- trench: 0.346\n",
      "- miles: 0.34\n",
      "- ordeal: 0.315\n",
      "- crevice: 0.309\n",
      "Closest words to the:\n",
      "- of: 0.64\n",
      "- and: 0.581\n",
      "- in: 0.53\n",
      "- a: 0.521\n",
      "- that: 0.499\n"
     ]
    }
   ],
   "source": [
    "for example in [\"above\", \"maid\", \"distance\", \"the\"]:\n",
    "    print(f\"Closest words to {example}:\")\n",
    "    for close_word in closest_words(example):\n",
    "        print(f\"- {close_word[0]}: {round(close_word[1], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPMI techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pointwise Mutual Information** (PPMI) measures how much two words co-occur more than expected by chance.\n",
    "\n",
    "For a word *w* and a context word *c*,\n",
    "\n",
    "$PMI(w,c) = log_2 \\frac{P(w,c)}{P(w)*P(c)}$\n",
    "\n",
    "But since we don't have the information of the real probability of words (corpus not infinite), we use the following PPMI estimation:\n",
    "\n",
    "$P(w_i, c_j) \\approx \\frac{C(w_i, c_j) + \\epsilon}{\\sum_{i=1}^V \\sum_{j=1}^C [C(w_i, c_j) + \\epsilon]}$\n",
    "\n",
    "$P(w_i) = \\sum_{j=1}^C P(w_i, c_j)$\n",
    "\n",
    "$P(c_j) = \\sum_{i=1}^V P(w_i, c_j)$\n",
    "\n",
    "$PPMI(w_i,c_j) = max( log_2 \\frac{P(w_i,c_j)}{P(w_i)*P(c_j)}, 0 )$\n",
    "\n",
    "With:\n",
    "- $C(w_i,c_j)$ the same as before\n",
    "- Vocabulary $V$ the set of words\n",
    "- Set of context words $C$ (maybe $C=V$)\n",
    "- Smoothing hyper-parameter $\\epsilon \\approx \\frac{1}{|V|}$ (e.g $\\epsilon = 10^{-4}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary == C(w_i, c_j)\n",
    "dictionary = {}\n",
    "# Compute V=C, the number of unique word, same as the set of context words\n",
    "V = 0\n",
    "for i in range(len(words)):\n",
    "    if dictionary.get(words[i], 0) == 0:\n",
    "        dictionary[words[i]] = {}\n",
    "        V+=1\n",
    "    for j in [i-2, i-1, i+1, i+2]:\n",
    "        if j not in [-2, -1, len(words), len(words)+1]:\n",
    "            dictionary[words[i]][words[j]] = dictionary[words[i]].get(words[j], 0)+1\n",
    "epsi = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_sum = 0\n",
    "for i in set(words):\n",
    "    for j in set(words):\n",
    "        double_sum += dictionary[i].get(j, 0) + epsi\n",
    "P_w_c = {}\n",
    "for i in set(words):\n",
    "    for j in set(words):\n",
    "        if P_w_c.get(i, 0) == 0:\n",
    "            P_w_c[i] = {}\n",
    "        P_w_c[i][j] = ( dictionary[i].get(j, 0) + epsi ) / double_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_w = {}\n",
    "for i in set(words):\n",
    "    temp = 0\n",
    "    for j in set(words):\n",
    "        temp += P_w_c[i][j]\n",
    "    P_w[i] = temp\n",
    "\n",
    "P_c = {}\n",
    "for j in set(words):\n",
    "    temp = 0\n",
    "    for i in set(words):\n",
    "        temp += P_w_c[i][j]\n",
    "    P_c[j] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(wi, cj):\n",
    "    return max(0, np.log2( P_w_c[wi][cj] / (P_w[wi] * P_c[cj]) ))\n",
    "\n",
    "# Give the top 5 closest word of the target\n",
    "# Words used in the same context\n",
    "def closest_words(target):\n",
    "    closest = []\n",
    "    for word in set(words):\n",
    "        closest.append( (word, PPMI(target, word)) )\n",
    "    closest = sorted(closest, key=lambda entry: -entry[1])\n",
    "    return closest[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words to above:\n",
      "- peculiarity: 10.683\n",
      "- canopy: 10.683\n",
      "- whisper: 9.221\n",
      "- heads: 8.254\n",
      "- fine: 8.036\n",
      "- shot: 7.68\n",
      "Closest words to maid:\n",
      "- sleeps: 10.683\n",
      "- scullery: 10.683\n",
      "- appointed: 9.774\n",
      "- oldmore: 9.774\n",
      "- chamber: 8.822\n",
      "- needs: 8.822\n",
      "Closest words to distance:\n",
      "- respectful: 9.886\n",
      "- decreased: 9.886\n",
      "- divided: 8.978\n",
      "- driving: 6.474\n",
      "- off: 5.822\n",
      "- gray: 5.607\n",
      "Closest words to the:\n",
      "- pony: 3.093\n",
      "- recess: 3.093\n",
      "- knight: 3.001\n",
      "- assassin: 3.001\n",
      "- maxillary: 3.001\n",
      "- gag: 3.001\n"
     ]
    }
   ],
   "source": [
    "for example in [\"above\", \"maid\", \"distance\", \"the\"]:\n",
    "    print(f\"Closest words to {example}:\")\n",
    "    for close_word in closest_words(example):\n",
    "        print(f\"- {close_word[0]}: {round(close_word[1], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Techniques\n",
    "\n",
    "The model represents each word as a dense vector, allowing for similarity calculations and capturing semantic relationships, making it useful for various NLP tasks such as word similarity, language translation, and document clustering. \n",
    "\n",
    "Word2Vec is typically trained using a shallow neural network to predict words in the context of other words, and the resulting word vectors can be used to capture semantic meaning and relationships in a way that is computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus: 60239\n",
      "There is 8 chunks of size: 7528\n"
     ]
    }
   ],
   "source": [
    "# Divide corpus into chunks\n",
    "n_chunks = 8\n",
    "chunk_size = (len(words)//n_chunks)-1\n",
    "print(f\"Size of corpus: {len(words)}\")\n",
    "print(f\"There is {n_chunks} chunks of size: {chunk_size}\")\n",
    "chunks = [words[i:i + chunk_size] for i in range(0, len(words), chunk_size)]\n",
    "# The training of the model is done during the initialisation of it\n",
    "model = Word2Vec(sentences=chunks, vector_size=100, window=2, min_count=2, \n",
    "                 workers=4, sg=1, negative=10, epochs=300, seed=42)\n",
    "\n",
    "# model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words to above:\n",
      "- whisper: 0.514\n",
      "- stars: 0.485\n",
      "- panelling: 0.471\n",
      "- silvered: 0.459\n",
      "- third: 0.446\n",
      "Closest words to maid:\n",
      "- torn: 0.471\n",
      "- chamber: 0.462\n",
      "- muffled: 0.451\n",
      "- cotton: 0.438\n",
      "- fields: 0.434\n",
      "Closest words to distance:\n",
      "- mile: 0.466\n",
      "- wheel: 0.452\n",
      "- burrow: 0.439\n",
      "- moonlight: 0.432\n",
      "- wound: 0.427\n",
      "Closest words to the:\n",
      "- of: 0.691\n",
      "- a: 0.574\n",
      "- and: 0.574\n",
      "- that: 0.485\n",
      "- i: 0.485\n"
     ]
    }
   ],
   "source": [
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "for example in [\"above\", \"maid\", \"distance\", \"the\"]:\n",
    "    print(f\"Closest words to {example}:\")\n",
    "    for close_word in model.wv.most_similar(example, topn=5):\n",
    "        print(f\"- {close_word[0]}: {round(close_word[1], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words to above:\n",
      "- below: 0.806\n",
      "- Above: 0.52\n",
      "- abover: 0.501\n",
      "- Instead_Jamines: 0.483\n",
      "- beyond: 0.481\n",
      "Closest words to maid:\n",
      "- housekeeper: 0.744\n",
      "- housemaid: 0.709\n",
      "- maids: 0.657\n",
      "- chambermaid: 0.635\n",
      "- maid_servant: 0.553\n",
      "Closest words to distance:\n",
      "- distances: 0.753\n",
      "- Distance: 0.554\n",
      "- withing_striking: 0.549\n",
      "- SMA##_remained_-##.##: 0.53\n",
      "- Distances: 0.512\n",
      "Closest words to the:\n",
      "- this: 0.594\n",
      "- in: 0.543\n",
      "- that: 0.526\n",
      "- ofthe: 0.515\n",
      "- another: 0.475\n"
     ]
    }
   ],
   "source": [
    "# With pretrained Word2Vec\n",
    "glove_vectors = downloader.load('word2vec-google-news-300')\n",
    "\n",
    "for example in [\"above\", \"maid\", \"distance\", \"the\"]:\n",
    "    print(f\"Closest words to {example}:\")\n",
    "    for close_word in glove_vectors.most_similar(example, topn=5):\n",
    "        print(f\"- {close_word[0]}: {round(close_word[1], 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
