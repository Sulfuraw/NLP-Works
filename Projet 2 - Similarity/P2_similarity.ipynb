{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity: Vector Semantics\n",
    "\n",
    "This project is about the similarity of words given a corpus that gives us a context for each word. From that similarity we can derive semantic meaning of words and further relationships between them.\n",
    "\n",
    "*The meaning of a word is its use in the language* -  Wittgenstein, 1953  \n",
    "Language use of a word can be characterized by counting occurence of other words around it.\n",
    "\n",
    "It involves the use of:\n",
    "\n",
    "- TF-IDF: The weighted frequency of words appearing together\n",
    "- PPMI: The measure of how much two words co-occur more than expected by chance\n",
    "- Word2Vec: Use a shallow neural network to predict words in the context of other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.lm import Vocabulary\n",
    "import numpy as np\n",
    "import os, sys\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# Change the current working directory to the location of the notebook\n",
    "os.chdir(os.getcwd())\n",
    "\n",
    "corpus = PlaintextCorpusReader(os.getcwd()+'/corpora', 'p2_sherlock\\.txt').words()\n",
    "vocab = Vocabulary(corpus, unk_cutoff=1)\n",
    "punct = [\"\\\"\", \"'\", \",\", \".\", \":\", \"?\", \"!\", \".\\\"\", \"?\\\"\", \"!\\\"\", \"-\", \",\\\"\", \"--\"]\n",
    "\n",
    "# Preprocessing\n",
    "words = []\n",
    "# test = {}\n",
    "for i in range(len(corpus)):\n",
    "    if corpus[i] not in punct:\n",
    "        words.append(corpus[i].lower())\n",
    "        # test[corpus[i].lower()] = test.get(corpus[i].lower(), 0)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF techniques\n",
    "\n",
    "$C(w_i, c_j)$:\n",
    "- The number of times context word cj occurs in the local contexts of target word wi\n",
    "- Forms the co-occurence matrix\n",
    "\n",
    "$tf_{i,j} = log_{10} (C(w_i, c_j) + 1)$:\n",
    "- Term frequency\n",
    "- Smoothed co-occurence frequency on a log scale\n",
    "\n",
    "$idf_j = log_{10} \\frac{N}{df_j}$:\n",
    "- $df_j$ the number of contextual windows of any target word where this context word $c_j$ occurs\n",
    "- N the total number of contextual windows for all target words\n",
    "- Words like \"the, a, it, etc...\" have a high document frequency and so, a low inverse document frequency\n",
    "\n",
    "TF-IDF: $w_{i,j} = tf_{i,j} \\times idf_j$\n",
    "- Weighted frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute C(w_i, c_j)  (dictionnary)\n",
    "# Via N-Gram 5 (2 before and 2 after the word). Could use nltk.ngrams(words, 5) to have the list of 5-gram\n",
    "# Accessing the contextual windows\n",
    "dictionary = {}\n",
    "# Compute df_j & N\n",
    "df = {}\n",
    "N = 0\n",
    "for i in range(len(words)):\n",
    "    if dictionary.get(words[i], 0) == 0:\n",
    "        dictionary[words[i]] = {}\n",
    "    N+=1\n",
    "    for j in [i-2, i-1, i+1, i+2]:\n",
    "        if j not in [-2, -1, len(words), len(words)+1]:\n",
    "            dictionary[words[i]][words[j]] = dictionary[words[i]].get(words[j], 0)+1\n",
    "            df[words[j]] = df.get(words[j], 0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sir': 2.6499717475814424,\n",
       " 'henry': 2.8043613034991757,\n",
       " 'three': 0.8045531714532216,\n",
       " 'broken': 1.4464836063347186,\n",
       " 'threads': 1.0472375255259798,\n",
       " 'hall': 3.542267416272485,\n",
       " 'the': 0.8359804946545215,\n",
       " 'charles': 2.7127583202582395,\n",
       " 'whose': 1.5380362693763716,\n",
       " 'sudden': 0.9857912880321348,\n",
       " 'was': 1.2143324830523035,\n",
       " 'written': 1.0032486035793737,\n",
       " 'and': 1.2348030417274845,\n",
       " 'in': 1.0291797662182496,\n",
       " 'family': 1.2451032785465193,\n",
       " 'but': 1.100649466082976,\n",
       " 'from': 0.8198972231791053,\n",
       " 'hugo': 2.513066165257388,\n",
       " 'as': 1.325302482073515,\n",
       " 'manor': 1.2576484629005042,\n",
       " 'of': 1.1147394838432119,\n",
       " 'held': 0.8596207943399203,\n",
       " 'near': 1.4062523195504442,\n",
       " 'estate': 0.9703928444215979,\n",
       " 'by': 0.8843610913923687,\n",
       " 'when': 0.964969018249066,\n",
       " 'he': 0.8489006436580457,\n",
       " 'for': 1.2088857214534718,\n",
       " 'passed': 0.8087038586721457,\n",
       " 'me': 0.7942161180589478,\n",
       " 'lying': 1.0472375255259798,\n",
       " 'on': 0.8602827422951178,\n",
       " 'which': 0.743203050701596,\n",
       " 'to': 1.1322674532909247,\n",
       " 'his': 0.9350203303433943,\n",
       " 'occurred': 0.9223181003856566,\n",
       " 'name': 1.2045428238961624,\n",
       " 'resided': 1.2576484629005042,\n",
       " 'at': 1.2580498716416133,\n",
       " 'residence': 1.1670294046110479,\n",
       " 'servants': 1.114020653661051,\n",
       " 'consisted': 1.2576484629005042,\n",
       " 'alley': 0.8951722297426782,\n",
       " 'tenant': 1.0472375255259798,\n",
       " 'it': 0.7061271898686518,\n",
       " 'mr': 1.134822032163642,\n",
       " 'if': 1.1209438396993987,\n",
       " 's': 1.408813145637887,\n",
       " 'younger': 1.706070034307995,\n",
       " 'i': 0.7792695083238416,\n",
       " 'must': 0.6737949906945135,\n",
       " 'motive': 1.0472375255259798,\n",
       " 'that': 1.1657263677215248,\n",
       " 'with': 1.1932009207302485,\n",
       " 'reach': 0.9566184672365232,\n",
       " 'within': 0.8174227999339668,\n",
       " 'this': 0.7855032555913286,\n",
       " 'demon': 1.114020653661051,\n",
       " 'who': 1.3991467827228656,\n",
       " 'arrives': 1.2576484629005042,\n",
       " 'rodger': 2.0944750510519596,\n",
       " 'youngest': 1.2576484629005042,\n",
       " 'old': 0.7060083128543391,\n",
       " 'masterful': 1.114020653661051,\n",
       " 'strain': 1.2576484629005042,\n",
       " 'every': 1.5783094556853696,\n",
       " 'goes': 0.9441580274028221,\n",
       " 'a': 1.012968668776438,\n",
       " 'is': 1.0748163015089056,\n",
       " 'then': 0.6008480970820044,\n",
       " 'you': 0.8776347650882971,\n",
       " 'death': 0.711907163011132,\n",
       " 'our': 0.890338749996956,\n",
       " 'breakfast': 0.9441580274028221,\n",
       " 'said': 1.6234004493861007,\n",
       " 'dr': 1.0193338134686682,\n",
       " 'northumberland': 1.562442225068542,\n",
       " 'hotel': 1.3182117936081834,\n",
       " 'perhaps': 0.8220104870004605,\n",
       " 'turned': 0.7629199108239091,\n",
       " 'someone': 0.8951722297426782,\n",
       " 'cut': 0.912629545289917,\n",
       " 'listened': 1.0472375255259798,\n",
       " 'go': 1.1415068418181165,\n",
       " 'why': 0.7139341131637652,\n",
       " 'mortimer': 0.6693628487576166,\n",
       " 'were': 1.1209438396993987,\n",
       " 'still': 0.7599819070470731,\n",
       " 'heard': 0.711907163011132,\n",
       " 'has': 0.5854496534714674,\n",
       " 'been': 0.8738307776018008,\n",
       " 'had': 0.8872360890829343,\n",
       " 'followed': 0.8316990420962,\n",
       " 'home': 0.8535389691133657,\n",
       " 'we': 0.7201820161394921,\n",
       " 'upstairs': 1.0472375255259798,\n",
       " 'one': 0.5373749094355261,\n",
       " 'himself': 0.7820862136609175,\n",
       " 'holmes': 0.5689515180595597,\n",
       " 'asked': 0.7753803506576101,\n",
       " 'what': 0.8853454691492445,\n",
       " 'barrymore': 0.6967323903978465,\n",
       " 'so': 0.5389725138295238,\n",
       " 'time': 0.6753064025106749,\n",
       " 'since': 0.751544420503225,\n",
       " 'answer': 0.8368265881514549,\n",
       " 'seized': 1.1670294046110479,\n",
       " 'down': 0.6407723704215476,\n",
       " 'depart': 1.2576484629005042,\n",
       " 'gave': 1.2955844851298295,\n",
       " 'inch': 1.0764103463215913,\n",
       " 'second': 0.8872464294332522,\n",
       " 'consulted': 1.2576484629005042,\n",
       " 'chapter': 1.4188144158290885,\n",
       " 'vi': 1.2576484629005042,\n",
       " 'between': 0.8005302181633689,\n",
       " 'young': 1.3624667238689068,\n",
       " 'upon': 0.8019874461514841,\n",
       " 'park': 1.1670294046110479,\n",
       " 'climate': 1.2576484629005042,\n",
       " 'stared': 0.932782537082138,\n",
       " 'eagerly': 1.0472375255259798,\n",
       " 'last': 0.6949536464149125,\n",
       " 'saw': 0.7202106304084369,\n",
       " 'dream': 1.114020653661051,\n",
       " 'sat': 0.8535389691133657,\n",
       " 'turn': 0.9036097162865264,\n",
       " 'an': 0.5636685501520341,\n",
       " 'sky': 0.9857912880321348,\n",
       " 'even': 0.8174227999339668,\n",
       " 'fell': 0.932782537082138,\n",
       " 'silent': 0.9223181003856566,\n",
       " 'whip': 1.2576484629005042,\n",
       " 'heads': 1.0234015953715945,\n",
       " 'shuddered': 1.2576484629005042,\n",
       " 'welcome': 1.0764103463215913,\n",
       " 'minds': 1.0234015953715945,\n",
       " 'wing': 1.114020653661051,\n",
       " 'experience': 0.9857912880321348,\n",
       " 'persecuting': 1.2576484629005042,\n",
       " 'no': 0.8371435409199269,\n",
       " 'way': 0.711907163011132,\n",
       " 'possibly': 0.8951722297426782,\n",
       " 'do': 0.5824267288303243,\n",
       " 'october': 1.0764103463215913,\n",
       " 'call': 0.8951722297426782,\n",
       " 'moor': 0.5909140289263104,\n",
       " 'oct': 1.2576484629005042,\n",
       " 'direction': 0.8596207943399203,\n",
       " 'towers': 1.706070034307995,\n",
       " 'rose': 0.8797737861321414,\n",
       " 'very': 0.5716752762863927,\n",
       " 'made': 1.159227987855812,\n",
       " 'some': 0.6106690427369098,\n",
       " 'be': 0.4995002224993877,\n",
       " 'there': 0.5064147617896242,\n",
       " 'off': 0.7820862136609175,\n",
       " 'leaving': 1.0764103463215913,\n",
       " 'gates': 1.114020653661051,\n",
       " 'are': 0.5649694132830855,\n",
       " 'rear': 1.2576484629005042,\n",
       " 'admiral': 1.2576484629005042,\n",
       " 'served': 1.0472375255259798,\n",
       " 'william': 1.2576484629005042,\n",
       " 'my': 0.45078707491930353,\n",
       " 'baronet': 0.751544420503225,\n",
       " 'mystery': 0.9703928444215979,\n",
       " 'recollection': 1.0764103463215913,\n",
       " 'indeed': 0.8129906579970699,\n",
       " 'perceive': 1.0764103463215913,\n",
       " 'stapleton': 0.6650760494326924,\n",
       " 'single': 0.9566184672365232,\n",
       " 'affair': 0.9441580274028221}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give tf-idf for a target words: Would correspond to wj, because i is fixed to the target\n",
    "def tf_idf_one(target):\n",
    "    closest = {}\n",
    "    for j in dictionary[target]:\n",
    "        tf_ij = np.log10( dictionary[target].get(j, 0) + 1 )\n",
    "        idf_j = np.log10( N / df.get(j, 0) )\n",
    "        w_ij = tf_ij*idf_j\n",
    "        closest[j] =  w_ij\n",
    "    return closest\n",
    "\n",
    "# Compute tf-idf weighted matrix: w\n",
    "tf_idf = {}\n",
    "for i in set(words):\n",
    "    tf_idf[i] = tf_idf_one(i)\n",
    "\n",
    "tf_idf[\"baskerville\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine simiarity between two words:\n",
    "\n",
    "$cos(v, w) = \\frac{\\sum_{i=1}^d v_i w_i}{\\sqrt{\\sum_{i=1}^d v_i^2} \\sqrt{\\sum_{i=1}^d w_i^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(word_1, word_2):\n",
    "    top_sum = 0\n",
    "    bot_left_sum = 0\n",
    "    bot_right_sum = 0\n",
    "    for word, value in tf_idf[word_1].items():\n",
    "        if word in tf_idf[word_2].keys():\n",
    "            top_sum += value*tf_idf[word_2][word]\n",
    "        bot_left_sum += value**2\n",
    "    for word_2, value_2 in tf_idf[word_2].items():\n",
    "        bot_right_sum += value_2**2\n",
    "    \n",
    "    bot_left_sum = np.sqrt(bot_left_sum)\n",
    "    bot_right_sum = np.sqrt(bot_right_sum)\n",
    "    return top_sum / (bot_left_sum * bot_right_sum)\n",
    "\n",
    "# Give the top 5 closest word of the target\n",
    "# Words used in the same context\n",
    "def closest_words(target):\n",
    "    closest = []\n",
    "    for word in set(words):\n",
    "        closest.append( (word, cosine_similarity(target, word)) )\n",
    "    closest = sorted(closest, key=lambda entry: -entry[1])\n",
    "\n",
    "    return closest[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words to above:\n",
      "- beams: 0.329\n",
      "- noticed: 0.253\n",
      "- velvet: 0.218\n",
      "- enables: 0.212\n",
      "- boars: 0.212\n",
      "Closest words to maid:\n",
      "- scion: 0.277\n",
      "- caretaker: 0.233\n",
      "- existence: 0.229\n",
      "- sketch: 0.227\n",
      "- desmonds: 0.217\n",
      "Closest words to distance:\n",
      "- grazier: 0.401\n",
      "- trench: 0.346\n",
      "- miles: 0.34\n",
      "- ordeal: 0.315\n",
      "- crevice: 0.309\n",
      "Closest words to the:\n",
      "- of: 0.64\n",
      "- and: 0.581\n",
      "- in: 0.53\n",
      "- a: 0.521\n",
      "- that: 0.499\n"
     ]
    }
   ],
   "source": [
    "for example in [\"above\", \"maid\", \"distance\", \"the\"]:\n",
    "    print(f\"Closest words to {example}:\")\n",
    "    for close_word in closest_words(example):\n",
    "        print(f\"- {close_word[0]}: {round(close_word[1], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPMI techniques\n",
    "\n",
    "**Pointwise Mutual Information** (PPMI) measures how much two words co-occur more than expected by chance.\n",
    "\n",
    "For a word *w* and a context word *c*,\n",
    "\n",
    "$PMI(w,c) = log_2 \\frac{P(w,c)}{P(w)*P(c)}$\n",
    "\n",
    "But since we don't have the information of the real probability of words (corpus not infinite), we use the following PPMI estimation:\n",
    "\n",
    "$P(w_i, c_j) \\approx \\frac{C(w_i, c_j) + \\epsilon}{\\sum_{i=1}^V \\sum_{j=1}^C [C(w_i, c_j) + \\epsilon]}$\n",
    "\n",
    "$P(w_i) = \\sum_{j=1}^C P(w_i, c_j)$\n",
    "\n",
    "$P(c_j) = \\sum_{i=1}^V P(w_i, c_j)$\n",
    "\n",
    "$PPMI(w_i,c_j) = max( log_2 \\frac{P(w_i,c_j)}{P(w_i)*P(c_j)}, 0 )$\n",
    "\n",
    "With:\n",
    "- $C(w_i,c_j)$ the same as before\n",
    "- Vocabulary $V$ the set of words\n",
    "- Set of context words $C$ (maybe $C=V$)\n",
    "- Smoothing hyper-parameter $\\epsilon \\approx \\frac{1}{|V|}$ (e.g $\\epsilon = 10^{-4}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 closest words to \"above\" with epsilon of 0,0001\n",
    "\n",
    "# 5 closest words to \"maid\" same epsilon\n",
    "\n",
    "# 5 closest words to \"distance\" same epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Techniques\n",
    "\n",
    "The model represents each word as a dense vector, allowing for similarity calculations and capturing semantic relationships, making it useful for various NLP tasks such as word similarity, language translation, and document clustering. \n",
    "\n",
    "Word2Vec is typically trained using a shallow neural network to predict words in the context of other words, and the resulting word vectors can be used to capture semantic meaning and relationships in a way that is computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('shone', 0.8492485284805298), ('moon', 0.7147756218910217), ('swung', 0.7015173435211182), ('escape', 0.6910266876220703), ('jaws', 0.6594571471214294)]\n",
      "[('oldmore', 0.6325182318687439), ('swung', 0.6313623785972595), ('lodge', 0.6225625276565552), ('offices', 0.6021811962127686), ('hounds', 0.5777137875556946)]\n",
      "[('divided', 0.6811988353729248), ('pace', 0.596568763256073), ('later', 0.5763490200042725), ('top', 0.5754624009132385), ('bit', 0.5565328001976013)]\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Vector Semantics (Word2Vec)\n",
    "# above - maid - distance\n",
    "corpus = PlaintextCorpusReader('/home/thomas/Bureau/LINFO2263/Project_2/corpora', 'p2_sherlock\\.txt').words() \n",
    "vocab = Vocabulary(corpus, unk_cutoff=1)\n",
    "punct = [\"\\\"\", \"'\", \",\", \".\", \":\", \"?\", \"!\", \".\\\"\", \"?\\\"\", \"!\\\"\", \"-\", \",\\\"\", \"--\"]\n",
    "\n",
    "# Preprocessing\n",
    "words = []\n",
    "for i in range(len(corpus)):\n",
    "    if corpus[i] not in punct:\n",
    "        words.append(corpus[i].lower())\n",
    "\n",
    "model = Word2Vec(sentences=[words], vector_size=100, window=2, min_count=2, sg=1, negative=10, epochs=300)\n",
    "most_sim_1 = model.wv.most_similar('above', topn=5)\n",
    "most_sim_2 = model.wv.most_similar('maid', topn=5)\n",
    "most_sim_3 = model.wv.most_similar('distance', topn=5)\n",
    "print(most_sim_1)\n",
    "print(most_sim_2)\n",
    "print(most_sim_3)\n",
    "\n",
    "# import gensim.downloader\n",
    "# glove_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "# most_sim = glove_vectors.most_similar(\"investigation\", topn=5)\n",
    "# print(most_sim)\n",
    "# [('investigations', 0.833749532699585), ('probe', 0.7943025827407837), ('inquiry', 0.7801670432090759), ('investgation', 0.6887422204017639), ('investigaton', 0.6771849989891052)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
